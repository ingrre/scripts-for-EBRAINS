{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking specimen to file bundles\n",
    "\n",
    "With this notebook you can do the following:\n",
    "1. Query file bundles and tissue sample collection metadata via the API for a particular dataset version(s)\n",
    "2. Link a specimen state to its corresponding file bundle\n",
    "3. Save the information to a CSV file so that it can be used for image ingestion\n",
    "\n",
    "**Note:** \\\n",
    "This script only works if:\n",
    "- Tissue sample collections have been added to a dataset version\n",
    "- Data has been uploaded to a container or bucket\n",
    "- A regex pattern (file structure pattern) has been defined and linked to repository so that file bundle can be created\n",
    "- File bundles have been created\n",
    "- The internal identifier of the tissue sample collection matches the file bundle name\n",
    "\n",
    "**If the script fails, please check if the points above.**\n",
    "\n",
    "To be able to run the script, you need to the following requirements:\n",
    "- Python version >= 3.6\n",
    "- read and write permission to the KG via the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "To interact with the API, you need an access token. To request a token, copy your token from the Knowledge Graph Editor or Query Builder (if you do not have access, request access via support@ebrains.eu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = getpass(prompt='Please paste your token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the dataset version\n",
    "\n",
    "First choose whether you want to extract tissue sample collection information from 1 or more dataset versions. If you only want to extract information from 1 dataset version, choose 1 for the first question and then fill in the UUID of the dataset version you are interested in.\n",
    "If you want to extract information from multiple dataset versions, choose 2 for the first question and then choose a keywords that exists in the title of all the dataset versions you are interested in. Make this as specific as possible to ensure that onlye the dataset versions of interest are queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First identify the dataset version\n",
    "\n",
    "dsv = input(\"What is the UUID of the dataset version? \")\n",
    "print(f\"The UUID of the dataset is: {dsv}\")\n",
    "cwd = os.getcwd()\n",
    "output_path = os.path.join(cwd, dsv)\n",
    "print(f\"The output folder is: {output_path}\")\n",
    "\n",
    "if os.path.isdir(output_path):\n",
    "    print(\"\\nOutput folder already exists\")\n",
    "else:\n",
    "    print(\"\\nOutput folder does not exist, making folder\")        \n",
    "    os.mkdir(output_path) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Knowledge Graph and extract information\n",
    "\n",
    "The following information will be extracted based on the dataset version UUID that you provided.\n",
    "- repository information\n",
    "- structure pattern (regex) used to create the file bundles for this dataset version\n",
    "- file bundle name, UUID, and grouping type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will extract important information, such as the DOI of the dataset, the tissue sample collections and the linked subjects.\n",
    "query = {\n",
    "  \"@context\": {\n",
    "    \"@vocab\": \"https://core.kg.ebrains.eu/vocab/query/\",\n",
    "    \"query\": \"https://schema.hbp.eu/myQuery/\",\n",
    "    \"propertyName\": {\n",
    "      \"@id\": \"propertyName\",\n",
    "      \"@type\": \"@id\"\n",
    "    },\n",
    "    \"path\": {\n",
    "      \"@id\": \"path\",\n",
    "      \"@type\": \"@id\"\n",
    "    }\n",
    "  },\n",
    "  \"meta\": {\n",
    "    \"type\": \"https://openminds.ebrains.eu/core/DatasetVersion\",\n",
    "    \"responseVocab\": \"https://schema.hbp.eu/myQuery/\"\n",
    "  },\n",
    "  \"structure\": [\n",
    "    {\n",
    "      \"propertyName\": \"query:id\",\n",
    "      \"path\": \"@id\",\n",
    "      \"required\": True,\n",
    "      \"filter\": {\n",
    "        \"op\": \"CONTAINS\",\n",
    "        \"value\": dsv\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"propertyName\": \"query:repository\",\n",
    "      \"path\": \"https://openminds.ebrains.eu/vocab/repository\",\n",
    "      \"structure\": [\n",
    "        {\n",
    "          \"propertyName\": \"query:name\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/name\",\n",
    "          \"required\": True\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:isPartOf\",\n",
    "          \"path\": {\n",
    "            \"@id\": \"https://openminds.ebrains.eu/vocab/isPartOf\",\n",
    "            \"reverse\": True\n",
    "          },\n",
    "          \"structure\": [\n",
    "            {\n",
    "              \"propertyName\": \"query:id\",\n",
    "              \"path\": \"@id\",\n",
    "              \"required\": True\n",
    "            },\n",
    "            {\n",
    "              \"propertyName\": \"query:name\",\n",
    "              \"path\": \"https://openminds.ebrains.eu/vocab/name\",\n",
    "              \"required\": True\n",
    "            },\n",
    "            {\n",
    "              \"propertyName\": \"query:groupingType\",\n",
    "              \"path\": \"https://openminds.ebrains.eu/vocab/groupingType\",\n",
    "              \"structure\": {\n",
    "                \"propertyName\": \"query:name\",\n",
    "                \"path\": \"https://openminds.ebrains.eu/vocab/name\",\n",
    "                \"required\": True\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:IRI\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/IRI\",\n",
    "          \"required\": True\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:structurePattern\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/structurePattern\",\n",
    "          \"structure\": [\n",
    "            {\n",
    "              \"propertyName\": \"query:lookupLabel\",\n",
    "              \"path\": \"https://openminds.ebrains.eu/vocab/lookupLabel\"\n",
    "            },\n",
    "            {\n",
    "              \"propertyName\": \"query:id\",\n",
    "              \"path\": \"@id\"\n",
    "            },\n",
    "            {\n",
    "              \"propertyName\": \"query:filePathPattern\",\n",
    "              \"path\": \"https://openminds.ebrains.eu/vocab/filePathPattern\",\n",
    "              \"structure\": {\n",
    "                \"propertyName\": \"query:regex\",\n",
    "                \"path\": \"https://openminds.ebrains.eu/vocab/regex\",\n",
    "                \"required\": True\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"propertyName\": \"query:studiedSpecimen\",\n",
    "      \"path\": \"https://openminds.ebrains.eu/vocab/studiedSpecimen\",\n",
    "      \"structure\": [\n",
    "        {\n",
    "          \"propertyName\": \"query:id\",\n",
    "          \"path\": \"@id\",\n",
    "          \"required\": True\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:internalIdentifier\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/internalIdentifier\",\n",
    "          \"required\": True\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:lookupLabel\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/lookupLabel\"\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:type\",\n",
    "          \"path\": \"@type\",\n",
    "          \"required\": True,\n",
    "          \"filter\": {\n",
    "            \"op\": \"CONTAINS\",\n",
    "            \"value\": \"TissueSampleCollection\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:studiedState\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/studiedState\",\n",
    "          \"required\": True,\n",
    "          \"structure\": {\n",
    "            \"propertyName\": \"query:id\",\n",
    "            \"path\": \"@id\",\n",
    "            \"required\": True\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Function to get the info based on what is defined in the query\n",
    "def getInfo(token, stage=\"IN_PROGRESS\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : string \n",
    "        Authentication token to access data and metadata in the KGE via the API\n",
    "    stage : string\n",
    "        Stage the data are in, e.g. \"RELEASED\", \"IN_PROGRESS\". Default is \"IN_PROGRESS\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dictionary\n",
    "        All data that was specified in the query\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "                \"Authorization\": \"Bearer \" + token\n",
    "              }\n",
    "\n",
    "    url = \"https://core.kg.ebrains.eu/v3-beta/queries/?vocab=https://schema.hbp.eu/myQuery/&stage={}\"\n",
    "    response = requests.post(url.format(stage), json=query, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(response, \"OK!\" )\n",
    "        data = response.json()\n",
    "    elif response.status_code == 401:\n",
    "        print(response, \"Token not valid, authorisation not successful\")\n",
    "        return\n",
    "    else:\n",
    "        print(response)\n",
    "        return\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the query we need to define the stage of release. If the data is under embargo, the stage is \"IN_PROGRESS\", if the data has already been released, the stage is \"RELEASED\". The default setting is \"in progress\" as it will find both released and ongoing curated metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stage\n",
    "stage = \"IN_PROGRESS\"\n",
    "\n",
    "# Execute the getInfo function\n",
    "result = getInfo(token, stage=stage)\n",
    "\n",
    "if result:\n",
    "    # extract file bundle info\n",
    "    fb_list = result[\"data\"][0][\"repository\"][0][\"isPartOf\"]\n",
    "    tsc_list = result[\"data\"][0][\"studiedSpecimen\"]\n",
    "\n",
    "    # Regex patterns used for this dataset\n",
    "    regex = result[\"data\"][0][\"repository\"][0][\"structurePattern\"][0][\"filePathPattern\"]\n",
    "    regex_list = []\n",
    "    for s in range(len(regex)):\n",
    "        if s == 0:\n",
    "            regex_list = regex[s][\"regex\"]\n",
    "        else:\n",
    "            regex_list = regex_list + \" \\n\" + regex[s][\"regex\"]\n",
    "\n",
    "\n",
    "    # Repository used in this dataset\n",
    "    repoName = result[\"data\"][0][\"repository\"][0][\"name\"]\n",
    "    repoIRI = result[\"data\"][0][\"repository\"][0][\"IRI\"]\n",
    "\n",
    "    print(f\"\\nNumber of tissue sample collections in this dataset: {len(tsc_list)}\")\n",
    "    print(f\"\\nNumber of file bundles in this dataset: {len(fb_list)}\")\n",
    "else:\n",
    "    print(\"\\nRefresh token and run the cells again!\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organise and save the metadata\n",
    "\n",
    "The metadata extracted by the query will now be organised into an easier to read format so that it can eventually be saved as a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract file bundle information\n",
    "def extractInfo(fb_list, regex_list, repoName, repoIRI):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    fb_list : list \n",
    "        Nested list of file bundle information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pandas DataFrame\n",
    "        Overview table with extracted information\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.DataFrame([])\n",
    "    for fb in fb_list:\n",
    "        data = data.append(pd.DataFrame({\"name\" : fb[\"name\"],\n",
    "                                        \"fileBundle_uuid\": fb[\"id\"].split(\"/\")[-1],\n",
    "                                        \"groupedBy\" : fb[\"groupingType\"][0][\"name\"],\n",
    "                                        \"repositoryName\" : repoName,\n",
    "                                        \"fromRepository\" : repoIRI,\n",
    "                                        \"regexPatternUsed\" : regex_list},                \n",
    "                                                index=[0]), \n",
    "                                ignore_index=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Extract relevant tissue sample metadata and add to the file bundle information\n",
    "def addInfo2fb(tsc_list, fb_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tsc_list : list \n",
    "        Nested list of tissue sample information information\n",
    "    fb_data : dataframe\n",
    "        DataFrame with file bundle information. Tissue sample information will be added to this DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fb_data : pandas DataFrame\n",
    "        Overview table with extracted information\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if not 'linkedSpecimenState' in fb_data.columns:\n",
    "        fb_data.insert(0, 'linkedSpecimenState', '')\n",
    "    for tsc in tsc_list:\n",
    "        state_atid = tsc[\"studiedState\"][0][\"id\"].split(\"/\")[-1]\n",
    "        tsc_id = tsc[\"internalIdentifier\"]\n",
    "        if tsc_id in fb_data.name.to_list():\n",
    "            idx = fb_data.index[fb_data.name == tsc_id][0]\n",
    "            fb_data.loc[idx, 'linkedSpecimenState'] = state_atid\n",
    "\n",
    "    print(\"Specimen information has been extracted and added to the file bundle information overview!\")\n",
    "    \n",
    "    return fb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the two functions above to extract the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata for file bundles first and then add more metadata about the tissue sample\n",
    "fb_data = extractInfo(fb_list, regex_list, repoName, repoIRI)\n",
    "fb_data = addInfo2fb(tsc_list, fb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link specimen states to file bundles\n",
    "\n",
    "Now you are interacting with the Knowledge Graph editor via the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link the specimen to their corresponding file bundles via the API\n",
    "def linkSpecimen2fb(token, fb_data):\n",
    "    kg_prefix = \"https://kg.ebrains.eu/api/instances/\"\n",
    "\n",
    "    hed = {'Authorization': 'Bearer ' + token}\n",
    "    url = \"https://core.kg.ebrains.eu/v3-beta/instances/{}?space=dataset\"\n",
    "\n",
    "    response = {}\n",
    "    for i in range(len(fb_data)):\n",
    "        fb_atid = fb_data.fileBundle_uuid[i]\n",
    "        fb_name = fb_data.name[i]\n",
    "        instance = {\"@context\": {\"@vocab\": \"https://openminds.ebrains.eu/vocab/\"},\n",
    "                    \"descendedFrom\": [{\"@id\" : kg_prefix + str(fb_data.linkedSpecimenState[i])}]\n",
    "                    }\n",
    "\n",
    "        \n",
    "        print(f\"Linking specimen to file bundle {fb_name} with uuid: {fb_atid}\")\n",
    "        response[fb_atid] = requests.patch(url.format(fb_atid), json=instance, headers=hed)\n",
    "        if response[fb_atid].status_code == 200:\n",
    "            print(response[fb_atid], \"OK!\" )\n",
    "        elif response[fb_atid].status_code == 401:\n",
    "            print(response[fb_atid], \"Token not valid, authorisation not successful\")\n",
    "            return\n",
    "        else:\n",
    "            print(response[fb_atid])\n",
    "            return\n",
    "\n",
    "    print(\"Specimen are now linked to the file bundles\")   \n",
    "\n",
    "    return fb_data     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After extracting and organising all the important metadata, you can link a tissue sample collection to its corresponding file bundle. Below you are asked whether you want to link the specimen or not. Type \"y\" if you want to create these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = input(\"Do you want to link the specimen states to the file bundles? Yes (y) or No (n): \")\n",
    "if answer == \"y\":\n",
    "    fb_data = linkSpecimen2fb(token, fb_data)\n",
    "else:\n",
    "    print(\"Specimen were not linked to the file bundles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the output file\n",
    "\n",
    "Your output file will be saved now and can be used for future reference. The name of the CSV file is \"fb_UUID-of-datasetVersion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(output_path, 'fb_' + dsv + '.csv')\n",
    "\n",
    "# save the table locally in the current folder using the name \"fb_UUID-of-dataset\"\n",
    "fb_data.to_csv(filename, index = False, header=True)\n",
    "\n",
    "print(\"Done! File is saved\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65747024ff2feda9d45d54fef14313a73f026ac80ef7ca2f452fcc1ca90b45f3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
