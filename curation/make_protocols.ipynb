{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Protocols and Protocol executions from file\n",
    "\n",
    "With this notebook you can generate a basic data provenance with protocols and protocols executions. This particular notebook considers a 3-step provenance starting with a subject and ending with a file (see figure)\n",
    "\n",
    "<img src=\"img/simpleProvenance.png\" width=\"800px\" height=\"250px\" align=\"center\"/>\n",
    "\n",
    "The steps in the notebook are as follows:\n",
    "1. Create protocols from an excel file\n",
    "2. Extract linked subject states, tissue sample states and file bundles from the KG and use them in the protocol executions as input and output\n",
    "3. Create protocol executions from file and link the generated protocols to it\n",
    "4. Post the newly created instances to the KGE\n",
    "\n",
    "To be able to run the script, you need to the following requirements:\n",
    "- Python version >= 3.6\n",
    "- openMINDS package (can be downloaded from https://pypi.org/project/openMINDS/)\n",
    "- read and write permission to the KG via the API\n",
    "\n",
    "A template file for the protocol and protocol execution is available as a macro-enabled excel file. If you enable the macros, you can use this file to select multiple options in the dropdown menu. When you have filled out the template, save it as an .xlsx file to remove the VBA so that it can imported in this notebook.\n",
    "\n",
    "Information about the protocols should be stored in the .xlsx file with the following column names written in sheet 'P'. \n",
    "- protocolName\n",
    "- protocolDescription\n",
    "- technique (dropdown of controlled instances)\n",
    "\n",
    "Information about the protocol executions should be stored in the same .xlsx file with the following column names written in sheet 'PE'.\n",
    "- protocolExecutionName\n",
    "- protocolExecutionDescription\n",
    "- preparationType (dropdown menu of controlled instances)\n",
    "- protocolUsed (dropdown menu from the protocol sheet)\n",
    "- inputType (dropdown menu of controlled instances)\n",
    "- input\n",
    "- outputType (dropdown menu of controlled instances)\n",
    "- output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important packages\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import openMINDS\n",
    "import openMINDS.version_manager\n",
    "\n",
    "# Initialise the openMINDS package version 3\n",
    "openMINDS.version_manager.init()\n",
    "openMINDS.version_manager.version_selection(\"v3\")\n",
    "helper = openMINDS.Helper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First specify the dataset version for which you want to create a data provenance and then give the name of the template file in which the information for the protocol and protocol executions are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Location of the files\n",
    "cwd = os.getcwd()\n",
    "\n",
    "answer = \"\"\n",
    "while answer not in [\"y\", \"n\"]: \n",
    "    answer = input(f\"Is this where your files are stored: {cwd}? yes (y) or no (n) \" ) \n",
    "    if answer == \"y\":\n",
    "        fpath = cwd\n",
    "        break\n",
    "    elif answer == \"n\":\n",
    "        fpath = input(\"Please define you path: \")\n",
    "        break\n",
    "\n",
    "dsv = input(\"What is the UUID of the dataset version? \")\n",
    "print(\"The UUID of the dataset is: \" + dsv)\n",
    "output_path = os.path.join(cwd, dsv)\n",
    "print('The output folder is: ' + dsv)\n",
    "\n",
    "protocol_file = input(\"What is name of the file with the protocol information? \")\n",
    "protocols = pd.read_excel(os.path.join(cwd, protocol_file + '.xlsx'), sheet_name = 'P')\n",
    "PEs = pd.read_excel(os.path.join(cwd, protocol_file + '.xlsx'), sheet_name = 'PE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Protocols\n",
    "\n",
    "Using the information from the template file in sheet 'P', we can now create the protocols using the openMINDS package.\n",
    "The generated instances are saved in a folder called 'protocol' and can be found in the output folder.\n",
    "\n",
    "An overview of the generated instances with their UUIDs is stored in the output folder as well with the name \"createdProtocols.csv\". This information will be used to link the newly generated protocols to the protocol executions that will be created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create protocol instances\n",
    "def createProtocols(protocols, output_path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    protocols : pandas DataFrame \n",
    "        Imported file excel file with information about the protocols\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas DataFrame\n",
    "        dataframe with relevant information about the created protocols\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    df = pd.DataFrame([])    \n",
    "    protocol_dict = {} \n",
    "    for p in range(len(protocols)):\n",
    "\n",
    "        mycol = helper.create_collection()\n",
    "        \n",
    "        # If more techniques were used, ensure that they are stored in the correct way\n",
    "        techniques = []\n",
    "        if protocols.technique[p].find(',') != -1:\n",
    "            for t in protocols.technique[p].split(\",\"):\n",
    "                techniques.append({\"@id\": \"https://openminds.ebrains.eu/instances/technique/\" + str(t.strip())})\n",
    "        else:\n",
    "            techniques = {\"@id\": \"https://openminds.ebrains.eu/instances/technique/\" + str(protocols.technique[p].strip())}\n",
    "            \n",
    "        # Create a protocol instance\n",
    "        protocol_dict[protocols.protocolName[p]] = mycol.add_core_protocol(name = protocols.protocolName[p],\n",
    "                                                                description = protocols.protocolDescription[p],\n",
    "                                                                technique = techniques\n",
    "                                                                )\n",
    "        \n",
    "        # Create an overview table with the important information\n",
    "        df = df.append(pd.DataFrame({\"type\" : \"protocol\",\n",
    "                        \"name\" : protocols.protocolName[p],\n",
    "                        \"description\" : protocols.protocolDescription[p],\n",
    "                        \"technique\" : protocols.technique[p],\n",
    "                        \"protocolAtid\" :  protocol_dict[protocols.protocolName[p]].split(\"/\")[-1]},                \n",
    "                    index=[0]),\n",
    "                ignore_index=True)\n",
    "        \n",
    "        # Save the openMINDS instance\n",
    "        mycol.save(os.path.join(output_path, \"\")) \n",
    "\n",
    "    print(\"Saving created instances...\")\n",
    "    \n",
    "    # Store the information in an overview file\n",
    "    filename = os.path.join(output_path, 'createdProtocols.csv')\n",
    "    df.to_csv(filename, index = False, header=True)  \n",
    "\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to create protocols\n",
    "createdProtocols = createProtocols(protocols, output_path)\n",
    "\n",
    "# print the overview file to ensure it was successful\n",
    "print(createdProtocols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Protocol Executions\n",
    "\n",
    "To create the protocol executions, we need to perform a couple of steps to ensure that most of the information is filled in automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information for input and outputs\n",
    "\n",
    "We will first query the Knowledge Graph Editor to extract information that could be used as the input and output of protocol execution steps. To ensure that this works, it is important that specimen and file bundles are already in the system and that they are linked in the correct way, with \"descendedFrom\".\n",
    "\n",
    "The query below will find the tissue sample collection of the dataset version that you specified earlier. It will also extract the subject and file bundle it is linked to, which allows you to create a data provenance as depicted in the image above.\n",
    "\n",
    "**Note**: If your data provenance deviates from the above example, you may not be able to link the correct input and output to the protocol executions. Nevertheless, the protocol executions will still be generated and you can make manual edits in the Knowledge graph editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will extract important information, such as the DOI of the dataset, the tissue sample collections and the linked subjects.\n",
    "query = {\n",
    "  \"@context\": {\n",
    "    \"@vocab\": \"https://core.kg.ebrains.eu/vocab/query/\",\n",
    "    \"query\": \"https://schema.hbp.eu/myQuery/\",\n",
    "    \"propertyName\": {\n",
    "      \"@id\": \"propertyName\",\n",
    "      \"@type\": \"@id\"\n",
    "    },\n",
    "    \"merge\": {\n",
    "      \"@type\": \"@id\",\n",
    "      \"@id\": \"merge\"\n",
    "    },\n",
    "    \"path\": {\n",
    "      \"@id\": \"path\",\n",
    "      \"@type\": \"@id\"\n",
    "    }\n",
    "  },\n",
    "  \"meta\": {\n",
    "    \"name\": \"get-dsv-specimen-fb\",\n",
    "    \"responseVocab\": \"https://schema.hbp.eu/myQuery/\",\n",
    "    \"type\": \"https://openminds.ebrains.eu/core/DatasetVersion\"\n",
    "  },\n",
    "  \"structure\": [\n",
    "    {\n",
    "      \"propertyName\": \"query:id\",\n",
    "      \"path\": \"@id\",\n",
    "      \"required\": True,\n",
    "      \"filter\": {\n",
    "        \"op\": \"CONTAINS\",\n",
    "        \"value\": dsv\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"propertyName\": \"query:studiedSpecimen\",\n",
    "      \"path\": \"https://openminds.ebrains.eu/vocab/studiedSpecimen\",\n",
    "      \"required\": True,\n",
    "      \"structure\": [\n",
    "        {\n",
    "          \"propertyName\": \"query:id\",\n",
    "          \"path\": \"@id\",\n",
    "          \"required\": True\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:lookupLabel\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/lookupLabel\",\n",
    "          \"required\": True\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:internalIdentifier\",\n",
    "          \"path\": \"https://openminds.ebrains.eu/vocab/internalIdentifier\"\n",
    "        },\n",
    "        {\n",
    "          \"propertyName\": \"query:type\",\n",
    "          \"path\": \"@type\",\n",
    "          \"required\": True,\n",
    "          \"filter\": {\n",
    "            \"op\": \"CONTAINS\",\n",
    "            \"value\": \"TissueSampleCollection\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "        \"propertyName\": \"query:studiedState\",\n",
    "        \"path\": \"https://openminds.ebrains.eu/vocab/studiedState\",\n",
    "        \"structure\": [\n",
    "          {\n",
    "            \"propertyName\": \"query:id\",\n",
    "            \"path\": \"@id\"\n",
    "          },\n",
    "          {\n",
    "            \"propertyName\": \"query:lookupLabel\",\n",
    "            \"path\": \"https://openminds.ebrains.eu/vocab/lookupLabel\"\n",
    "          },\n",
    "          {\n",
    "            \"propertyName\": \"query:descendedFromFile\",\n",
    "            \"path\": {\n",
    "              \"@id\": \"https://openminds.ebrains.eu/vocab/descendedFrom\",\n",
    "              \"reverse\": True\n",
    "            },\n",
    "            \"structure\": [\n",
    "              {\n",
    "                \"propertyName\": \"query:id\",\n",
    "                \"path\": \"@id\"\n",
    "              },\n",
    "              {\n",
    "                \"propertyName\": \"query:name\",\n",
    "                \"path\": \"https://openminds.ebrains.eu/vocab/name\"\n",
    "              }\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"propertyName\": \"query:descendedFromSubject\",\n",
    "            \"path\": \"https://openminds.ebrains.eu/vocab/descendedFrom\",\n",
    "            \"structure\": [\n",
    "              {\n",
    "              \"propertyName\": \"query:id\",\n",
    "              \"path\": \"@id\"\n",
    "              },\n",
    "              {\n",
    "              \"propertyName\": \"query:lookupLabel\",\n",
    "              \"path\": \"https://openminds.ebrains.eu/vocab/lookupLabel\"\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Function to get the info based on what is defined in the query\n",
    "def getInfo(token, stage=\"IN_PROGRESS\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : string \n",
    "        Authentication token to access data and metadata in the KGE via the API\n",
    "    stage : string\n",
    "        Stage the data are in, e.g. \"RELEASED\", \"IN_PROGRESS\". Default is \"IN_PROGRESS\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dictionary\n",
    "        All data that was specified in the query\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"accept\": \"*/*\",\n",
    "        \"Authorization\": \"Bearer \" + token\n",
    "        }\n",
    "\n",
    "    url = \"https://core.kg.ebrains.eu/v3-beta/queries/?vocab=https://schema.hbp.eu/myQuery/&stage={}\"\n",
    "    response = requests.post(url.format(stage), json=query, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(response, \"OK! Query as successful!\" )\n",
    "        data = response.json()\n",
    "    elif response.status_code == 401:\n",
    "        print(response, \"Token not valid, authorisation not successful\")\n",
    "        return\n",
    "    else:\n",
    "        print(response)\n",
    "        return\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "To interact with the API, you need an access token. To request a token, copy your token from the Knowledge Graph Editor or Query Builder (if you do not have access, request access via support@ebrains.eu).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = getpass(prompt='Please paste your token: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the token, we will now execute the query and extract information from the KGE. The default state of release of the data is set to \"IN_PROGRESS\". You can change this to \"RELEASED if you want to.\n",
    "\n",
    "**Note:** If you get a 401 error, it indicates that your token is not valid and may be expired. Refresh the browser where you extracted the token from, run the authetication cell again and past the new token in the input cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = getInfo(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract tissue sample collection information\n",
    "def extractInfo(tsc_list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tsc_list : list \n",
    "        Nested list of tsc information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pandas DataFrame\n",
    "        Overview table with extracted information\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.DataFrame([])\n",
    "    for tsc in tsc_list:\n",
    "        if not \"descendedFromFile\" in tsc['studiedState'][0].keys():\n",
    "            fileBundle_name = \"\"\n",
    "            fileBundle_uuid = \"\"\n",
    "        else:\n",
    "            if tsc[\"studiedState\"][0][\"descendedFromFile\"] == []:\n",
    "                fileBundle_name = \"\"\n",
    "                fileBundle_uuid = \"\"\n",
    "            else:\n",
    "                fileBundle_name = tsc[\"studiedState\"][0][\"descendedFromFile\"][0][\"name\"]\n",
    "                fileBundle_uuid = tsc[\"studiedState\"][0][\"descendedFromFile\"][0][\"id\"].split(\"/\")[-1]\n",
    "\n",
    "\n",
    "        data = data.append(pd.DataFrame({\"subject_state_name\" : tsc[\"studiedState\"][0][\"descendedFromSubject\"][0][\"lookupLabel\"],\n",
    "                        \"subject_state_uuid\" : tsc[\"studiedState\"][0][\"descendedFromSubject\"][0][\"id\"].split(\"/\")[-1],\n",
    "                        \"tsc_state_name\" : tsc[\"studiedState\"][0][\"lookupLabel\"],\n",
    "                        \"tsc_state_uuid\" : tsc[\"studiedState\"][0][\"id\"].split(\"/\")[-1],\n",
    "                        \"fileBundle_name\" : fileBundle_name,  \n",
    "                        \"fileBundle_uuid\" : fileBundle_uuid},                \n",
    "                    index=[0]), \n",
    "                ignore_index=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the query we only use the specimen to extract information from \n",
    "tsc_list = data[\"data\"][0][\"studiedSpecimen\"]\n",
    "extractedData = extractInfo(tsc_list)\n",
    "\n",
    "# Print the extracted data so that you know if the query was successful and all the information is available.\n",
    "print(extractedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the instances for protocol executions\n",
    "\n",
    "Using the information extracted from the KGE, the newly created protocol instances and the information you defined in the excel file, we can now create the procotol execution instances.\n",
    "\n",
    "The function below will try to find the correct input or output of the protocol executions based on the information is available. If an input or output cannot be found, it will state this when you run the cell. If you think this is a mistake, please check whether the names in the extracted data table above matches the names you entered in the excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making protocol executions based on newly made protocol instances\n",
    "def makeProtocolExecutions(createdProtocols, extractedData, PEs, dsv, output_path):\n",
    "\n",
    "    df = pd.DataFrame([])  \n",
    "    protocolEx_dict = {} \n",
    "    protocolNames = createdProtocols.name.tolist()\n",
    "    for p in range(len(PEs)):\n",
    "\n",
    "        mycol = helper.create_collection()\n",
    "\n",
    "        # If more than one protocol was used, ensure that it is formatted correctly\n",
    "        protocolAtid = []\n",
    "        protocolIDs = []\n",
    "        if PEs.protocolUsed[p].find(',') != -1:\n",
    "            for t in PEs.protocolUsed[p].split(\", \"):\n",
    "                if t in protocolNames:\n",
    "                    protocol_atid = createdProtocols.protocolAtid[protocolNames.index(t)]\n",
    "                    protocolAtid.append({\"@id\": \"https://kg.ebrains.eu/api/instances/\" + str(protocol_atid)})\n",
    "                    protocolIDs.append(protocol_atid)\n",
    "        else:\n",
    "            if PEs.protocolUsed[p] in protocolNames:\n",
    "                protocol_atid = createdProtocols.protocolAtid[protocolNames.index(PEs.protocolUsed[p])]\n",
    "                protocolAtid = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + str(protocol_atid)}\n",
    "                protocolIDs = protocol_atid\n",
    "            \n",
    "        \n",
    "        # Find the corresponding input and outputs\n",
    "        if PEs.inputType[p] == \"subject state\":\n",
    "            if PEs.input[p] in extractedData.subject_state_name.to_list():\n",
    "                input = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + \n",
    "                extractedData.subject_state_uuid.to_list()[extractedData.subject_state_name.to_list().index(PEs.input[p])]}\n",
    "            else:\n",
    "                input = None\n",
    "                print(f\"No input found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "\n",
    "        if PEs.inputType[p] == \"tsc state\":\n",
    "            if PEs.input[p] in extractedData.tsc_state_name.to_list():\n",
    "                input = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + \n",
    "                extractedData.tsc_state_uuid.to_list()[extractedData.tsc_state_name.to_list().index(PEs.input[p])]}\n",
    "            else:\n",
    "                input = None\n",
    "                print(f\"No input found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "        \n",
    "        if PEs.inputType[p] == \"fileBundle\":\n",
    "            if PEs.input[p] in extractedData.fileBundle_name.to_list():\n",
    "                input = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + \n",
    "                extractedData.fileBundle_uuid.to_list()[extractedData.fileBundle_name.to_list().index(PEs.input[p])]}\n",
    "            else:\n",
    "                input = None\n",
    "                print(f\"No input found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "\n",
    "        if PEs.outputType[p] == \"subject state\":\n",
    "            if PEs.output[p] in extractedData.subject_state_name.to_list():\n",
    "                output = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + \n",
    "                extractedData.subject_state_uuid.to_list()[extractedData.subject_state_name.to_list().index(PEs.output[p])]}\n",
    "            else:\n",
    "                output = None\n",
    "                print(f\"No output found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "\n",
    "        if PEs.outputType[p] == \"tsc state\":\n",
    "            if PEs.output[p] in extractedData.tsc_state_name.to_list():\n",
    "                output = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + \n",
    "                extractedData.tsc_state_uuid.to_list()[extractedData.tsc_state_name.to_list().index(PEs.output[p])]}\n",
    "            else:\n",
    "                output = None\n",
    "                print(f\"No output found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "        \n",
    "        if PEs.outputType[p] == \"fileBundle\":\n",
    "            if PEs.output[p] in extractedData.fileBundle_name.to_list():\n",
    "                output = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + \n",
    "                extractedData.fileBundle_uuid.to_list()[extractedData.fileBundle_name.to_list().index(PEs.output[p])]}\n",
    "            else:\n",
    "                output = None\n",
    "                print(f\"No output found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "        \n",
    "        if PEs.inputType[p] == \"file\":\n",
    "            input = None\n",
    "            print(f\"No input found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "        elif PEs.outputType[p] == \"file\":\n",
    "            output = None\n",
    "            print(f\"No output found for protocol execution {PEs.protocolExecutionName[p]}\")\n",
    "        \n",
    "        # Create the protocol execution instances\n",
    "        protocolEx_dict[PEs.protocolExecutionName[p]] = mycol.add_core_protocolExecution(input = input,\n",
    "                                                                                            output = output,\n",
    "                                                                                            protocol = protocolAtid,\n",
    "                                                                                            isPartOf = {\"@id\": \"https://kg.ebrains.eu/api/instances/\" + dsv})\n",
    "        \n",
    "        mycol.get(protocolEx_dict[PEs.protocolExecutionName[p]]).preparationDesign = {\"@id\": \"https://openminds.ebrains.eu/instances/preparationType/\" + PEs.preparationType[p]}\n",
    "        mycol.get(protocolEx_dict[PEs.protocolExecutionName[p]]).lookupLabel = PEs.protocolExecutionName[p]\n",
    "        mycol.get(protocolEx_dict[PEs.protocolExecutionName[p]]).description = PEs.protocolExecutionDescription[p]\n",
    "        \n",
    "        # Save the intances\n",
    "        mycol.save(os.path.join(output_path, \"\")) \n",
    "\n",
    "        # Create an overview file\n",
    "        df = df.append(pd.DataFrame({\"PE_name\" : PEs.protocolExecutionName[p],\n",
    "                \"PE_uuid\" : protocolEx_dict[PEs.protocolExecutionName[p]].split(\"/\")[-1],\n",
    "                \"input\" : input,\n",
    "                \"output\" : output,\n",
    "                \"protocol\" : PEs.protocolUsed[p],  \n",
    "                \"protocol_uuid\" : \", \".join(protocolIDs)},                \n",
    "            index=[0]), \n",
    "        ignore_index=True)\n",
    "\n",
    "    print(\"Saving created instances...\")\n",
    "    \n",
    "    # Store information in an overview file\n",
    "    filename = os.path.join(output_path, 'createdProtocolExecutions.csv')\n",
    "    df.to_csv(filename, index = False, header=True)  \n",
    "\n",
    "    print(\"Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function to create protocol executions\n",
    "df = makeProtocolExecutions(createdProtocols, extractedData, PEs, dsv, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload instances to the KGE\n",
    "\n",
    "When you have created all the instances you want to create, you can upload them the the Knowledge Graph editor.\n",
    "\n",
    "Ensure that the token is still up to date. In case your token is expired, you will receive a message to update your token. Go back to the authorisation cell and run ONLY that cell again to refresh your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload the instances to the KGE\n",
    "def upload(instances_fnames, token, space_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    instances_fnames : List \n",
    "        list of file paths to instances that need to be uploaded\n",
    "    token : string\n",
    "        Authorisation token to get access to the KGE\n",
    "    space_name : string\n",
    "        Space that the instances needs to be uploaded to, e.g. \"dataset\", \"common\", etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response : dictionary\n",
    "        For each UUID as response is stored that indications if the upload \n",
    "        was successful\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    hed = {\"accept\": \"*/*\",\n",
    "           \"Authorization\": \"Bearer \" + token,\n",
    "           \"Content-Type\": \"application/json\"\n",
    "           }\n",
    "    \n",
    "    # Prefix to upload to the right space\n",
    "    url = \"https://core.kg.ebrains.eu/v3-beta/instances/{}?space=\" + space_name\n",
    "    kg_prefix = \"https://kg.ebrains.eu/api/instances/\"\n",
    "    \n",
    "    new_instances = []\n",
    "    for fname in instances_fnames:\n",
    "        with open(fname, 'r') as f:\n",
    "            new_instances.append(json.load(f))\n",
    "        f.close()\n",
    "    \n",
    "    # Correct the capitalisation in the openMINDS package\n",
    "    for instance in new_instances:\n",
    "        atid = kg_prefix + instance[\"@id\"].split(\"/\")[-1] #only take the UUID \n",
    "        instance[\"@id\"] = atid\n",
    "        if instance[\"@type\"].endswith(\"Protocolexecution\"):\n",
    "            splittype = instance[\"@type\"].split(\"/\")[:-1]\n",
    "            splittype.append(\"ProtocolExecution\")\n",
    "            instance[\"@type\"] = \"/\".join(splittype)\n",
    "\n",
    "    # Upload to the KGE\n",
    "    print(\"\\nUploading instances now:\\n\")\n",
    "    \n",
    "    count = 0\n",
    "    response = {}    \n",
    "    for instance in new_instances:\n",
    "        count += 1\n",
    "        print(\"Posting instance \" + str(count) + \"/\" + str(len(new_instances)))\n",
    "        atid = instance[\"@id\"].split(\"/\")[-1]  \n",
    "        response[atid] = requests.post(url.format(atid), json=instance, headers=hed)\n",
    "        if response[atid].status_code == 200:\n",
    "            print(response[atid], \"OK!\" )\n",
    "        elif response[atid].status_code == 409:\n",
    "            print(response[atid], \"Instance already exists\")\n",
    "        elif response[atid].status_code == 401:\n",
    "            print(response[atid], \"Token not valid, authorisation not successful\")\n",
    "        else:\n",
    "            print(response[atid])\n",
    "        \n",
    "    return response  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload instances to the KGE\n",
    "answer = input(\"Would you like to upload the instances you created to the KGE? yes (y) or no (n) \" ) \n",
    "\n",
    "if answer == \"y\":\n",
    "    instances_fnames = glob.glob(os.path.join(output_path,\"\") + \"*\\\\*\", recursive = True)\n",
    "    \n",
    "    if token != \"\":\n",
    "        if instances_fnames == []:\n",
    "            print(\"No files found\")\n",
    "        else: \n",
    "            response = upload(instances_fnames, token, space_name = \"dataset\")  \n",
    "        \n",
    "elif answer == \"n\":\n",
    "    print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "65747024ff2feda9d45d54fef14313a73f026ac80ef7ca2f452fcc1ca90b45f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
